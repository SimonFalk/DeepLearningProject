{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from data_wrangling.datamanager import DataLoader as DL\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import scipy.signal as sig\n",
    "import sys\n",
    "import pickle\n",
    "from welford import Welford"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_in = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nperseg = 100\n",
    "noverlap = 99\n",
    "nhop = nperseg - noverlap\n",
    "logfft = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DL()\n",
    "sessions = ['s1', 's4', 's5', 's6', 's7', 's10']\n",
    "data = dl.get_fcx2(sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectrogram_base(\n",
    "        signal,\n",
    "        nperseg = 100,\n",
    "        noverlap = 99,\n",
    "        nfft = 2 ** 8,\n",
    "        f_lo=10,\n",
    "        f_hi=100,\n",
    "        window = 'boxcar'\n",
    "    ):\n",
    "        freqs, times, spect = sig.spectrogram(\n",
    "            signal,\n",
    "            1000,\n",
    "            window,\n",
    "            nperseg = nperseg,\n",
    "            noverlap = noverlap,\n",
    "            nfft = nfft,\n",
    "            scaling = 'spectrum'\n",
    "        )\n",
    "        freqs_idxs = (freqs >= f_lo) & (freqs <= f_hi)\n",
    "        spect = spect[freqs_idxs].T\n",
    "        spect = np.sqrt(spect)\n",
    "\n",
    "        return freqs, freqs_idxs, times, spect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving data from session s1...\n",
      "...finished\n",
      "Retrieving data from session s4...\n",
      "...finished\n",
      "Retrieving data from session s5...\n",
      "...finished\n",
      "Retrieving data from session s6...\n",
      "...finished\n",
      "Retrieving data from session s7...\n",
      "...finished\n",
      "Retrieving data from session s10...\n",
      "...finished\n"
     ]
    }
   ],
   "source": [
    "def get_as_array(session):\n",
    "    print(\"Retrieving data from session %s...\"%session)\n",
    "    spects = []\n",
    "    arrs = data[session]['data']\n",
    "    mn = arrs.mean(0).reshape((1, -1))\n",
    "    st = arrs.std (0).reshape((1, -1))\n",
    "    arrs -= mn\n",
    "    arrs /= st\n",
    "    print(\"...finished\")\n",
    "    return arrs\n",
    "\n",
    "\n",
    "Xs = []\n",
    "spect_params = []\n",
    "for s in sessions:\n",
    "    X = get_as_array(s)\n",
    "    Xs.append(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[73, 66, 172, 91, 110, 110]\n"
     ]
    }
   ],
   "source": [
    "n_channelss = [X.shape[1] for X in Xs]\n",
    "print(n_channelss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[      0,  651776,  651876,  912366,  912466, 1303752],\n",
       "       [      0,  488509,  488609,  683792,  683892,  977218],\n",
       "       [      0,  508799,  508899,  712198,  712298, 1017799],\n",
       "       [      0,  419299,  419399,  586898,  586998,  838799],\n",
       "       [      0,  381299,  381399,  533698,  533798,  762799],\n",
       "       [      0,  407746,  407846,  570724,  570824,  815692]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = 0\n",
    "# Indexing/setup\n",
    "\n",
    "train_frac = 0.5\n",
    "val_frac   = 0.2\n",
    "\n",
    "train_ranges = np.array([\n",
    "    (\n",
    "        0,\n",
    "        int(X.shape[0] * train_frac) - t_in - nperseg\n",
    "    )\n",
    "    for X in Xs\n",
    "])\n",
    "\n",
    "val_ranges = np.array([\n",
    "    (\n",
    "        train_max + t_in, \n",
    "        train_max + int(X.shape[0] * val_frac) - t_in - nperseg\n",
    "    )\n",
    "    for (_, train_max), X in zip(train_ranges, Xs)\n",
    "])\n",
    "\n",
    "test_ranges = np.array([\n",
    "    (\n",
    "        val_max + t_in,\n",
    "        len(X) - t_in - nperseg\n",
    "    )\n",
    "    for (val_min, val_max), X in zip(val_ranges, Xs)\n",
    "])\n",
    "\n",
    "np.concatenate([\n",
    "    train_ranges,\n",
    "    val_ranges,\n",
    "    test_ranges\n",
    "], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generator\n",
    "\n",
    "n_sessions = len(sessions)\n",
    "\n",
    "def get_random_data_idxs(sess_no, n, mode = 'train'):\n",
    "    if mode == 'train':\n",
    "        _idxs = np.arange(*list(train_ranges[sess_no].copy()))\n",
    "    elif mode == 'val':\n",
    "        _idxs = np.arange(*list(val_ranges[sess_no].copy()))\n",
    "    elif mode == 'test':\n",
    "        _idxs = np.arange(*list(test_ranges[sess_no].copy()))\n",
    "        \n",
    "    np.random.shuffle(_idxs)\n",
    "    channels = np.random.choice(n_channelss[sess_no], size=n, replace=True)\n",
    "    return np.array([\n",
    "            _idxs[:n], \n",
    "            _idxs[:n] + t_in + nperseg - nhop, \n",
    "            channels\n",
    "    ]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(\n",
    "    batch_size,\n",
    "    steps_per_epoch,\n",
    "    epochs,\n",
    "    normalize = False,\n",
    "    mode = 'train'\n",
    "):\n",
    "    \n",
    "    for _ in range(steps_per_epoch * epochs):\n",
    "        \n",
    "        # Select session indices\n",
    "        # number of samples for each session\n",
    "        counts = pd.Series(\n",
    "            np.random.randint(n_sessions, size = batch_size)\n",
    "        ).value_counts()\n",
    "\n",
    "        # Session-wise indices\n",
    "        # [[start, end, chan] per session]\n",
    "        idxs = [\n",
    "            get_random_data_idxs(idx, n, mode = mode)\n",
    "            for idx, n in enumerate(counts)\n",
    "        ]\n",
    "        \n",
    "        \n",
    "        samples = np.concatenate([\n",
    "            np.array([\n",
    "                X[lo : hi, chan] \n",
    "                for lo, hi, chan in idx\n",
    "            ])\n",
    "            for X, idx in zip(Xs, idxs)\n",
    "        ])\n",
    "        \n",
    "        # Alt 1. False\n",
    "        if normalize == False:\n",
    "            samples = np.stack([\n",
    "                spectrogram_base(\n",
    "                    samp,\n",
    "                    nperseg,\n",
    "                    noverlap,\n",
    "                    2 ** logfft\n",
    "                )[3]\n",
    "                for samp in samples\n",
    "            ])\n",
    "            \n",
    "        # Alt 2. Mean, std tuple\n",
    "        else:\n",
    "            mn, st = normalize\n",
    "            samples = (np.stack([\n",
    "                spectrogram_base(\n",
    "                    samp,\n",
    "                    nperseg,\n",
    "                    noverlap,\n",
    "                    2 ** logfft\n",
    "                )[3]\n",
    "                for samp in samples\n",
    "            ]) - mn) / st\n",
    "        \n",
    "        yield (samples, samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "welford_from_generator = Welford()\n",
    "w_epochs = 5\n",
    "w_batch_size = 5000\n",
    "# 5 * t_in * 5000 = 12.5M\n",
    "for x, _ in data_generator(w_batch_size, 1, w_epochs):\n",
    "    for spects in x:\n",
    "        welford_from_generator.add_all(spects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mn, st = welford_from_generator.mean.reshape((1, -1)), np.sqrt(welford_from_generator.var_p).reshape((1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional (Bidirectional (None, 100, 128)          45056     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "repeat_vector (RepeatVector) (None, 100, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100, 128)          131584    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 100, 128)          98816     \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 100, 128)          98816     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100, 23)           2967      \n",
      "=================================================================\n",
      "Total params: 476,055\n",
      "Trainable params: 476,055\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Input((t_in, 23)),\n",
    "    layers.Bidirectional(\n",
    "        layers.LSTM(64, return_sequences = True)\n",
    "    ),\n",
    "    layers.Bidirectional(\n",
    "        layers.LSTM(64)\n",
    "    ),\n",
    "    layers.RepeatVector(t_in),\n",
    "    layers.LSTM(128, return_sequences = True),\n",
    "    layers.Bidirectional(\n",
    "        layers.LSTM(64, return_sequences = True)\n",
    "    ),\n",
    "    layers.Bidirectional(\n",
    "        layers.LSTM(64, return_sequences = True)\n",
    "    ),\n",
    "    layers.Dense(23)\n",
    "])\n",
    "model.compile(loss = 'mse', optimizer = 'adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "100/100 [==============================] - 60s 538ms/step - loss: 0.6501 - val_loss: 0.4508\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 52s 523ms/step - loss: 0.3829 - val_loss: 0.3787\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 52s 521ms/step - loss: 0.3471 - val_loss: 0.3368\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 52s 521ms/step - loss: 0.3135 - val_loss: 0.3112\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 52s 519ms/step - loss: 0.3076 - val_loss: 0.3174\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 51s 518ms/step - loss: 0.2775 - val_loss: 0.2846\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 52s 519ms/step - loss: 0.2589 - val_loss: 0.3578\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 52s 519ms/step - loss: 0.2528 - val_loss: 0.2677\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 52s 519ms/step - loss: 0.2389 - val_loss: 0.2664\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 51s 518ms/step - loss: 0.2340 - val_loss: 0.2458\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 51s 516ms/step - loss: 0.2255 - val_loss: 0.2562\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 52s 520ms/step - loss: 0.2224 - val_loss: 0.2390\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 52s 520ms/step - loss: 0.2201 - val_loss: 0.2206\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 51s 517ms/step - loss: 0.2047 - val_loss: 0.2200\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 52s 519ms/step - loss: 0.2021 - val_loss: 0.2114\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 53s 534ms/step - loss: 0.1987 - val_loss: 0.2119\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 55s 554ms/step - loss: 0.1946 - val_loss: 0.2159\n",
      "Epoch 1/100\n",
      "300/300 [==============================] - 41s 137ms/step - loss: 0.2752 - val_loss: 0.3943\n",
      "Epoch 2/100\n",
      "300/300 [==============================] - 41s 136ms/step - loss: 0.2431 - val_loss: 0.2392\n",
      "Epoch 3/100\n",
      "300/300 [==============================] - 39s 129ms/step - loss: 0.2023 - val_loss: 0.2100\n",
      "Epoch 4/100\n",
      "300/300 [==============================] - 38s 127ms/step - loss: 0.2256 - val_loss: 0.2091\n",
      "Epoch 5/100\n",
      "300/300 [==============================] - 38s 128ms/step - loss: 0.2128 - val_loss: 0.2238\n",
      "Epoch 6/100\n",
      "300/300 [==============================] - 38s 127ms/step - loss: 0.1950 - val_loss: 0.2043\n",
      "Epoch 7/100\n",
      "300/300 [==============================] - 38s 128ms/step - loss: 0.1832 - val_loss: 0.2008\n",
      "Epoch 8/100\n",
      "300/300 [==============================] - 38s 128ms/step - loss: 0.1931 - val_loss: 0.2069\n",
      "Epoch 9/100\n",
      "300/300 [==============================] - 38s 128ms/step - loss: 0.1801 - val_loss: 0.2056\n",
      "Epoch 10/100\n",
      "300/300 [==============================] - 38s 127ms/step - loss: 0.1988 - val_loss: 0.2017\n"
     ]
    }
   ],
   "source": [
    "# Training part 1\n",
    "\n",
    "batch_size      = 1024\n",
    "steps_per_epoch = 100\n",
    "epochs          = 100\n",
    "\n",
    "validation_batch_size     = batch_size\n",
    "vaidation_steps_per_epoch = 30\n",
    "# Necessary for function\n",
    "validation_epochs         = 1\n",
    "\n",
    "hist1 = model.fit(\n",
    "    # Training data and configuration\n",
    "    x = data_generator(\n",
    "        batch_size, \n",
    "        steps_per_epoch, \n",
    "        epochs,\n",
    "        normalize = (mn, st)\n",
    "    ),\n",
    "    batch_size      = batch_size,\n",
    "    steps_per_epoch = steps_per_epoch,\n",
    "    epochs          = epochs,\n",
    "    \n",
    "    # Validation data\n",
    "    validation_data = tf.data.Dataset.from_generator(\n",
    "        lambda: data_generator(\n",
    "            validation_batch_size, \n",
    "            vaidation_steps_per_epoch, \n",
    "            validation_epochs,\n",
    "            mode = 'val',\n",
    "            normalize = (mn, st)\n",
    "        ),\n",
    "        output_signature = (\n",
    "            tf.TensorSpec(shape = (validation_batch_size, t_in, 23), dtype = tf.float64),\n",
    "            tf.TensorSpec(shape = (validation_batch_size, t_in, 23), dtype = tf.float64),\n",
    "        )\n",
    "    ),\n",
    "    validation_batch_size = validation_batch_size,\n",
    "    validation_steps      = vaidation_steps_per_epoch,\n",
    "    \n",
    "    callbacks = [keras.callbacks.EarlyStopping(\n",
    "        patience = 2,\n",
    "        min_delta = 1e-3\n",
    "    )]\n",
    ")\n",
    "\n",
    "batch_size      = 128\n",
    "steps_per_epoch = 300\n",
    "epochs          = 100\n",
    "\n",
    "hist2 = model.fit(\n",
    "    # Training data and configuration\n",
    "    x = data_generator(\n",
    "        batch_size, \n",
    "        steps_per_epoch, \n",
    "        epochs,\n",
    "        normalize = (mn, st)\n",
    "    ),\n",
    "    batch_size      = batch_size,\n",
    "    steps_per_epoch = steps_per_epoch,\n",
    "    epochs          = epochs,\n",
    "    \n",
    "    # Validation data\n",
    "    validation_data = tf.data.Dataset.from_generator(\n",
    "        lambda: data_generator(\n",
    "            validation_batch_size, \n",
    "            vaidation_steps_per_epoch, \n",
    "            validation_epochs,\n",
    "            mode = 'val',\n",
    "            normalize = (mn, st)\n",
    "        ),\n",
    "        output_signature = (\n",
    "            tf.TensorSpec(shape = (validation_batch_size, t_in, 23), dtype = tf.float64),\n",
    "            tf.TensorSpec(shape = (validation_batch_size, t_in, 23),     dtype = tf.float64),\n",
    "        )\n",
    "    ),\n",
    "    validation_batch_size = validation_batch_size,\n",
    "    validation_steps      = vaidation_steps_per_epoch,\n",
    "    \n",
    "    callbacks = [keras.callbacks.EarlyStopping(\n",
    "        patience = 3,\n",
    "        min_delta = 5e-4\n",
    "    )]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 369s 368ms/step - loss: 0.2220\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.22196829319000244"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = model.evaluate(\n",
    "    x = data_generator(\n",
    "        1024,\n",
    "        1000,\n",
    "        1,\n",
    "        mode = 'test',\n",
    "        normalize = (mn, st)\n",
    "    )\n",
    ")\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_4 (Bidirection (None, 100, 128)          45056     \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 100, 128)          98816     \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 100, 128)          98816     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100, 23)           2967      \n",
      "=================================================================\n",
      "Total params: 245,655\n",
      "Trainable params: 245,655\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Input((t_in, 23)),\n",
    "    layers.Bidirectional(\n",
    "        layers.LSTM(64, return_sequences = True)\n",
    "    ),\n",
    "    layers.Bidirectional(\n",
    "        layers.LSTM(64, return_sequences = True)\n",
    "    ),\n",
    "    layers.Bidirectional(\n",
    "        layers.LSTM(64, return_sequences = True)\n",
    "    ),\n",
    "    layers.Dense(23)\n",
    "])\n",
    "model.compile(loss = 'mse', optimizer = 'adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "100/100 [==============================] - 55s 515ms/step - loss: 0.4624 - val_loss: 0.1313\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 50s 506ms/step - loss: 0.0917 - val_loss: 0.0333\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 50s 506ms/step - loss: 0.0317 - val_loss: 0.0543\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 50s 505ms/step - loss: 0.0226 - val_loss: 0.0337\n",
      "Epoch 1/100\n",
      "300/300 [==============================] - 37s 124ms/step - loss: 0.0150 - val_loss: 0.0277\n",
      "Epoch 2/100\n",
      "300/300 [==============================] - 37s 124ms/step - loss: 0.0081 - val_loss: 0.0141\n",
      "Epoch 3/100\n",
      "300/300 [==============================] - 37s 123ms/step - loss: 0.0053 - val_loss: 0.0114\n",
      "Epoch 4/100\n",
      "300/300 [==============================] - 37s 123ms/step - loss: 0.0118 - val_loss: 0.0127\n",
      "Epoch 5/100\n",
      "300/300 [==============================] - 37s 123ms/step - loss: 0.0043 - val_loss: 0.0076\n",
      "Epoch 6/100\n",
      "300/300 [==============================] - 37s 124ms/step - loss: 0.0140 - val_loss: 0.0181\n",
      "Epoch 7/100\n",
      "300/300 [==============================] - 37s 124ms/step - loss: 0.0039 - val_loss: 0.0084\n",
      "Epoch 8/100\n",
      "300/300 [==============================] - 37s 123ms/step - loss: 0.0110 - val_loss: 0.0095\n"
     ]
    }
   ],
   "source": [
    "# Training part 1\n",
    "\n",
    "batch_size      = 1024\n",
    "steps_per_epoch = 100\n",
    "epochs          = 100\n",
    "\n",
    "validation_batch_size     = batch_size\n",
    "vaidation_steps_per_epoch = 30\n",
    "# Necessary for function\n",
    "validation_epochs         = 1\n",
    "\n",
    "hist1 = model.fit(\n",
    "    # Training data and configuration\n",
    "    x = data_generator(\n",
    "        batch_size, \n",
    "        steps_per_epoch, \n",
    "        epochs,\n",
    "        normalize = (mn, st)\n",
    "    ),\n",
    "    batch_size      = batch_size,\n",
    "    steps_per_epoch = steps_per_epoch,\n",
    "    epochs          = epochs,\n",
    "    \n",
    "    # Validation data\n",
    "    validation_data = tf.data.Dataset.from_generator(\n",
    "        lambda: data_generator(\n",
    "            validation_batch_size, \n",
    "            vaidation_steps_per_epoch, \n",
    "            validation_epochs,\n",
    "            mode = 'val',\n",
    "            normalize = (mn, st)\n",
    "        ),\n",
    "        output_signature = (\n",
    "            tf.TensorSpec(shape = (validation_batch_size, t_in, 23), dtype = tf.float64),\n",
    "            tf.TensorSpec(shape = (validation_batch_size, t_in, 23), dtype = tf.float64),\n",
    "        )\n",
    "    ),\n",
    "    validation_batch_size = validation_batch_size,\n",
    "    validation_steps      = vaidation_steps_per_epoch,\n",
    "    \n",
    "    callbacks = [keras.callbacks.EarlyStopping(\n",
    "        patience = 2,\n",
    "        min_delta = 1e-3\n",
    "    )]\n",
    ")\n",
    "\n",
    "batch_size      = 128\n",
    "steps_per_epoch = 300\n",
    "epochs          = 100\n",
    "\n",
    "hist2 = model.fit(\n",
    "    # Training data and configuration\n",
    "    x = data_generator(\n",
    "        batch_size, \n",
    "        steps_per_epoch, \n",
    "        epochs,\n",
    "        normalize = (mn, st)\n",
    "    ),\n",
    "    batch_size      = batch_size,\n",
    "    steps_per_epoch = steps_per_epoch,\n",
    "    epochs          = epochs,\n",
    "    \n",
    "    # Validation data\n",
    "    validation_data = tf.data.Dataset.from_generator(\n",
    "        lambda: data_generator(\n",
    "            validation_batch_size, \n",
    "            vaidation_steps_per_epoch, \n",
    "            validation_epochs,\n",
    "            mode = 'val',\n",
    "            normalize = (mn, st)\n",
    "        ),\n",
    "        output_signature = (\n",
    "            tf.TensorSpec(shape = (validation_batch_size, t_in, 23), dtype = tf.float64),\n",
    "            tf.TensorSpec(shape = (validation_batch_size, t_in, 23),     dtype = tf.float64),\n",
    "        )\n",
    "    ),\n",
    "    validation_batch_size = validation_batch_size,\n",
    "    validation_steps      = vaidation_steps_per_epoch,\n",
    "    \n",
    "    callbacks = [keras.callbacks.EarlyStopping(\n",
    "        patience = 3,\n",
    "        min_delta = 5e-4\n",
    "    )]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 366s 365ms/step - loss: 0.0157\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.015673896297812462"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = model.evaluate(\n",
    "    x = data_generator(\n",
    "        1024,\n",
    "        1000,\n",
    "        1,\n",
    "        mode = 'test',\n",
    "        normalize = (mn, st)\n",
    "    )\n",
    ")\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_7 (Bidirection (None, 100, 128)          45056     \n",
      "_________________________________________________________________\n",
      "bidirectional_8 (Bidirection (None, 100, 128)          98816     \n",
      "_________________________________________________________________\n",
      "bidirectional_9 (Bidirection (None, 100, 128)          98816     \n",
      "_________________________________________________________________\n",
      "bidirectional_10 (Bidirectio (None, 100, 128)          98816     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100, 23)           2967      \n",
      "=================================================================\n",
      "Total params: 344,471\n",
      "Trainable params: 344,471\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Input((t_in, 23)),\n",
    "    layers.Bidirectional(\n",
    "        layers.LSTM(64, return_sequences = True)\n",
    "    ),\n",
    "    layers.Bidirectional(\n",
    "        layers.LSTM(64, return_sequences = True)\n",
    "    ),\n",
    "    layers.Bidirectional(\n",
    "        layers.LSTM(64, return_sequences = True)\n",
    "    ),\n",
    "    layers.Bidirectional(\n",
    "        layers.LSTM(64, return_sequences = True)\n",
    "    ),\n",
    "    layers.Dense(23)\n",
    "])\n",
    "model.compile(loss = 'mse', optimizer = 'adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "100/100 [==============================] - 57s 525ms/step - loss: 0.4671 - val_loss: 0.2075\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 51s 514ms/step - loss: 0.1017 - val_loss: 0.0574\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 51s 512ms/step - loss: 0.0386 - val_loss: 0.0290\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 51s 509ms/step - loss: 0.0208 - val_loss: 0.0252\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 51s 513ms/step - loss: 0.0159 - val_loss: 0.0232\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 51s 512ms/step - loss: 0.0144 - val_loss: 0.0274\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 51s 512ms/step - loss: 0.0138 - val_loss: 0.0144\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 51s 511ms/step - loss: 0.0140 - val_loss: 0.0783\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 51s 511ms/step - loss: 0.0069 - val_loss: 0.0239\n",
      "Epoch 1/100\n",
      "300/300 [==============================] - 38s 126ms/step - loss: 0.0221 - val_loss: 0.0155\n",
      "Epoch 2/100\n",
      "300/300 [==============================] - 38s 126ms/step - loss: 0.0106 - val_loss: 0.0096\n",
      "Epoch 3/100\n",
      "300/300 [==============================] - 38s 126ms/step - loss: 0.0062 - val_loss: 0.0141\n",
      "Epoch 4/100\n",
      "300/300 [==============================] - 37s 125ms/step - loss: 0.0083 - val_loss: 0.0260\n",
      "Epoch 5/100\n",
      "300/300 [==============================] - 38s 125ms/step - loss: 0.0035 - val_loss: 0.0115\n"
     ]
    }
   ],
   "source": [
    "# Training part 1\n",
    "\n",
    "batch_size      = 1024\n",
    "steps_per_epoch = 100\n",
    "epochs          = 100\n",
    "\n",
    "validation_batch_size     = batch_size\n",
    "vaidation_steps_per_epoch = 30\n",
    "# Necessary for function\n",
    "validation_epochs         = 1\n",
    "\n",
    "hist1 = model.fit(\n",
    "    # Training data and configuration\n",
    "    x = data_generator(\n",
    "        batch_size, \n",
    "        steps_per_epoch, \n",
    "        epochs,\n",
    "        normalize = (mn, st)\n",
    "    ),\n",
    "    batch_size      = batch_size,\n",
    "    steps_per_epoch = steps_per_epoch,\n",
    "    epochs          = epochs,\n",
    "    \n",
    "    # Validation data\n",
    "    validation_data = tf.data.Dataset.from_generator(\n",
    "        lambda: data_generator(\n",
    "            validation_batch_size, \n",
    "            vaidation_steps_per_epoch, \n",
    "            validation_epochs,\n",
    "            mode = 'val',\n",
    "            normalize = (mn, st)\n",
    "        ),\n",
    "        output_signature = (\n",
    "            tf.TensorSpec(shape = (validation_batch_size, t_in, 23), dtype = tf.float64),\n",
    "            tf.TensorSpec(shape = (validation_batch_size, t_in, 23), dtype = tf.float64),\n",
    "        )\n",
    "    ),\n",
    "    validation_batch_size = validation_batch_size,\n",
    "    validation_steps      = vaidation_steps_per_epoch,\n",
    "    \n",
    "    callbacks = [keras.callbacks.EarlyStopping(\n",
    "        patience = 2,\n",
    "        min_delta = 1e-3\n",
    "    )]\n",
    ")\n",
    "\n",
    "batch_size      = 128\n",
    "steps_per_epoch = 300\n",
    "epochs          = 100\n",
    "\n",
    "hist2 = model.fit(\n",
    "    # Training data and configuration\n",
    "    x = data_generator(\n",
    "        batch_size, \n",
    "        steps_per_epoch, \n",
    "        epochs,\n",
    "        normalize = (mn, st)\n",
    "    ),\n",
    "    batch_size      = batch_size,\n",
    "    steps_per_epoch = steps_per_epoch,\n",
    "    epochs          = epochs,\n",
    "    \n",
    "    # Validation data\n",
    "    validation_data = tf.data.Dataset.from_generator(\n",
    "        lambda: data_generator(\n",
    "            validation_batch_size, \n",
    "            vaidation_steps_per_epoch, \n",
    "            validation_epochs,\n",
    "            mode = 'val',\n",
    "            normalize = (mn, st)\n",
    "        ),\n",
    "        output_signature = (\n",
    "            tf.TensorSpec(shape = (validation_batch_size, t_in, 23), dtype = tf.float64),\n",
    "            tf.TensorSpec(shape = (validation_batch_size, t_in, 23),     dtype = tf.float64),\n",
    "        )\n",
    "    ),\n",
    "    validation_batch_size = validation_batch_size,\n",
    "    validation_steps      = vaidation_steps_per_epoch,\n",
    "    \n",
    "    callbacks = [keras.callbacks.EarlyStopping(\n",
    "        patience = 3,\n",
    "        min_delta = 5e-4\n",
    "    )]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 366s 365ms/step - loss: 0.0137\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.013722263276576996"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = model.evaluate(\n",
    "    x = data_generator(\n",
    "        1024,\n",
    "        1000,\n",
    "        1,\n",
    "        mode = 'test',\n",
    "        normalize = (mn, st)\n",
    "    )\n",
    ")\n",
    "mse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python36964bite8c2268c4d7745c494921b3032b5f046"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
